{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a9e712",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "###### course :https://www.educative.io/courses/langchain-llm/\n",
    "\n",
    "###### which lib is for what\n",
    "- Use langchain_core for building blocks and logic.\n",
    "- Use langchain_community for connectors, loaders, and community tools.\n",
    "- Use langchain_groq for Groq-hosted LLMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7d429",
   "metadata": {},
   "source": [
    "##### Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95e21ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fa18453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "groq_key = os.getenv('GROQ_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cb3742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model='llama3-8b-8192', #'deepseek-r1-distill-llama-70b',#llama-3.3-70b-versatile\n",
    "               api_key=groq_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11c76bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cefc936",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": \"Hi, my name is Alex.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cd63301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to assist you. How can I help you today? Do you have a specific question, need help with a task, or would you like to discuss a particular topic? I'm here to listen and provide information to the best of my abilities.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1f74b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an AI, so I don't have personal updates or knowledge that is limited to a specific time frame. I was trained on a massive dataset of text from the internet and can provide information up to a certain point in time, but I don't have the ability to receive or be aware of updates in real-time.\\n\\nMy training data is a snapshot of the internet from a specific point in time, and it includes a vast amount of information on various topics, including news, science, technology, history, and more. However, my knowledge may not reflect the very latest developments or updates, especially in rapidly changing fields like technology or current events.\\n\\nIf you have a specific question or topic you'd like to know more about, I'll do my best to provide you with accurate and relevant information based on my training data.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"what is the last update you have?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e79ca",
   "metadata": {},
   "source": [
    "##### using roles\n",
    "- system\n",
    "- user\n",
    "- assistant\n",
    "- tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a9ebe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq.chat_models import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32b83a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow, I\\'m just blown away by the complexity of your question. I mean, seriously, who wouldn\\'t know that 2*3 is... (drumroll please)... 6?! It\\'s not like it\\'s rocket science or anything. I\\'m just amazed that you managed to come up with such a challenging question. Can I give you a trophy for \"Most Obvious Math Problem Ever\"?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='you are a math teacher who answers with a sarcasam'),\n",
    "    HumanMessage('what is 2*3')\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab4ca3",
   "metadata": {},
   "source": [
    "##### Using Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b5f0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0335373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s the invitation email:\\n\\nSubject: You\\'re Invited: Exciting Launch of Our Latest GenAI Product!\\n\\nDear [Recipient],\\n\\nKHOSLA SAAAB!\\n\\nWe\\'re thrilled to invite you to the most anticipated event of the year - the launch of our latest GenAI product! Join us at the Grand Ballroom, City Center Hotel on January 15, 2024, at 11 AM, as we unveil the future of innovation.\\n\\nGet ready to witness an exciting unveiling of our latest GenAI product, packed with cutting-edge features and technologies that will revolutionize the way we live and work. Our team has worked tirelessly to bring you the most exciting and game-changing product, and we can\\'t wait to share it with you!\\n\\nAt this event, you\\'ll have the opportunity to:\\n\\n* Be among the first to experience our latest GenAI product\\n* Learn about its features and benefits\\n* Connect with our team and other industry leaders\\n* Enjoy a special launch celebration with refreshments and networking opportunities\\n\\nDon\\'t miss this chance to be a part of the excitement! Mark your calendars for January 15, 2024, at 11 AM, and get ready to join the GenAI revolution!\\n\\nDate: January 15, 2024\\nTime: 11 AM\\nLocation: Grand Ballroom, City Center Hotel\\n\\nRSVP by January 10, 2024, to secure your spot. Simply reply to this email with a \"YES\" or \"NO\" to confirm your attendance.\\n\\nWe look forward to seeing you there!\\n\\nBest regards,\\n\\n[Your Name]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_template =  PromptTemplate.from_template(\"Create an invitation email to the recipient that is {recipient_name}\\\n",
    "for an event that is {event_type}\\\n",
    "in a language that is {language}\\\n",
    "Mention the event location that is {event_location}\\\n",
    "and event date that is {event_date}.\\\n",
    "Also write few sentences about the event description that is {event_description}\\\n",
    "in style that is {style}.\")\n",
    "\n",
    "details = {\"recipient_name\":\"KHOSLA SAAAB !\",\n",
    "  \"event_type\":\"product launch\",\n",
    "  \"language\": \"American english\",\n",
    "  \"event_location\":\"Grand Ballroom, City Center Hotel\",\n",
    "  \"event_date\":\"11 AM, January 15, 2024\",\n",
    "  \"event_description\":\"an exciting unveiling of our latest GenAI product\",\n",
    "  \"style\":\"enthusiastic tone\"}\n",
    "\n",
    "prompt_value = email_template.invoke(details)\n",
    "response = llm.invoke(prompt_value)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9751f3",
   "metadata": {},
   "source": [
    "##### Parsing Outputs with LangChain\n",
    "\n",
    "LLMs typically output a string of text. However, when creating an LLM-powered application, we might need a more structured, formatted output that delivers concise information instead of requiring us to read the complete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c75dae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a0d1d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Answer the question - \\n Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 0754-09-06T20:27:39.045837Z, 1136-08-11T02:42:37.758224Z, 1084-08-30T12:53:06.260189Z\\n\\nReturn ONLY this string, no other words!\\nWhen was the iPhone released\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2007, 1, 9, 18, 0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser_dateTime = DatetimeOutputParser()\n",
    "\n",
    "prompt_datetime = PromptTemplate.from_template(\n",
    "    template=\"Answer the question - \\n {instruction}\\n{question}\",\n",
    "    input_vairables = [\"question\"],\n",
    "    partial_variables = {\"instruction\": parser_dateTime.get_format_instructions()},\n",
    "    \n",
    ")\n",
    "print(prompt_datetime.invoke({\"question\": \"When was the iPhone released\"}))\n",
    "prompt_value = prompt_datetime.invoke({\"question\": \"When was the iPhone released\"})\n",
    "response = llm.invoke(prompt_value)\n",
    "returned_object = parser_dateTime.parse(response.content)\n",
    "returned_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9d9f0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first iPhone was released on June 29, 2007. It was announced by Steve Jobs, the co-founder and CEO of Apple, at the Macworld conference in San Francisco on January 9, 2007. The iPhone was released in the United States exclusively on the AT&T network and was available in a 4GB, 8GB, or 16GB model. It was priced at $499 for the 4GB model, $599 for the 8GB model, and $699 for the 16GB model.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"When was the iPhone released\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "476f35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import JsonOutputKeyToolsParser, CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "885a64ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are the top 5 phones in no particular order',\n",
       " 'separated by commas:',\n",
       " 'Samsung Galaxy S22 Ultra',\n",
       " 'Apple iPhone 13 Pro',\n",
       " 'Google Pixel 6 Pro',\n",
       " 'OnePlus 9 Pro',\n",
       " 'Huawei P40 Pro']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "comma_template = PromptTemplate.from_template(\n",
    "    template=\"answer the question -  \\n {instruction}\\n{question}\",\n",
    "    input_vairables = [\"question\"],\n",
    "    partial_variables = {\"instruction\": comma_parser.get_format_instructions()}\n",
    "\n",
    ")\n",
    "\n",
    "prompt_value = comma_template.invoke({\"question\":\"list me best 5 phones\"})\n",
    "response = llm.invoke(prompt_value)\n",
    "comma_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e2ad7",
   "metadata": {},
   "source": [
    "##### pydantic parser\n",
    "So far, we’ve seen some examples of output parsers that convert the input string to a single specific format. To extract multiple fields from the output string of an LLM, we can use the PydanticOutputParser. This parser parses the response into a defined TypedDict class, JSON schema, or a Pydantic class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad006b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel,Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1e1b0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Authors(name='Dan Brown', number=17, books=['Digital Fortress', 'Angels & Demons', 'The Da Vinci Code', 'The Lost Symbol', 'Inferno', 'Origin', 'Wild Symphony', 'The Institute', 'The Last Symbol', 'Pactum', 'Wild Game', 'The Omega Factor', 'Robert Langdon', 'Harvard Symbologist', 'The Solomon Key', 'Deception Point', \"The Vatican's Secret\", 'The Solomon Key'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Authors(BaseModel):\n",
    "    name:str = Field(description=\"name of author\")\n",
    "    number:int = Field(description=\"number of book by author\")\n",
    "    books:list[str] = Field(description=\"list of books\")\n",
    "\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Authors)\n",
    "\n",
    "prompt_list = PromptTemplate.from_template(\n",
    "    template = \"Answer the question.\\n{format_instructions}\\n{question}\",\n",
    "    input_vairables = [\"question\"],\n",
    "    partial_variables = {\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt_value = prompt_list.invoke({\"question\":\"list me books of Dan Brown\"})\n",
    "\n",
    "response = llm.invoke(prompt_value)\n",
    "pydantic_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31a20d",
   "metadata": {},
   "source": [
    "##### .with_structured_output() method\n",
    "The .with_structured_output() method is an easier and simpler way to get a structured output from the model. This is supported by most models and greatly reduces the complexity of getting a structured output. Let’s modify our existing code to use this new method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8935a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10eead590>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10eeadf90>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6053103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Authors(name='Dan Brown', number=1, books=['Angels & Demons', 'The Da Vinci Code', 'The Lost Symbol', 'Inferno', 'Origin', 'The Institute'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Authors(BaseModel):\n",
    "    name:str = Field(description=\"name of author\")\n",
    "    number:int = Field(description=\"number of book by author\")\n",
    "    books:list[str] = Field(description=\"list of books\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Authors)\n",
    "response  = structured_llm.invoke(\"list me books of Dan Brown\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db5199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c47a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_agentic_kn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
