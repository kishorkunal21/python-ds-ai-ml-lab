{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5e3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## source : https://www.educative.io/courses/langchain-llm/runnables-and-expression-language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08f63d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kunal21/opt/miniconda3/envs/v_agentic_kn/bin/python\n"
     ]
    }
   ],
   "source": [
    "! which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ed0bca",
   "metadata": {},
   "source": [
    "### Runnables in LangChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef823b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "GROQ_KEY = os.getenv('GROQ_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7c743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53555ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model='llama3-8b-8192', #'deepseek-r1-distill-llama-70b',#llama-3.3-70b-versatile\n",
    "               api_key=GROQ_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f00d2",
   "metadata": {},
   "source": [
    "##### Runnables -   Callable unit of work\n",
    "Common methods ; \n",
    "1. invoke() processes a single input and returns a single output, ideal for individual requests.\n",
    "2. batch () allows us to process a list of inputs simultaneously and return a list of corresponding outputs, which is much more efficient for handling multiple requests at once.\n",
    "3. stream() processes a single input but returns an output as a stream of chunks, which is useful for displaying real-time progress or handling very large outputs.\n",
    "\n",
    "- LECL - langchain expressions language : for chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3996eba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x1080b1fd0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1080b2ba0>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d179f9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would classify this feedback as Positive. The phrase \"delivery was on time\" suggests that the customer is satisfied with the delivery process and the product was received as expected, which is a positive experience.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## simple chain example - input->promptTemplate->parser->output\n",
    "## finding setiment as positive or negative \n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "sentiment_template = PromptTemplate(\n",
    "    input_variables=[\"Feedback\"],\n",
    "    template=\"Determine sentiment of this Feedback as Positive or Negative\\n{Feedback}\"\n",
    ")\n",
    "\n",
    "user_feedback = \"delivery was on time \"\n",
    "\n",
    "chain = sentiment_template | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({'Feedback':user_feedback})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043bd4f1",
   "metadata": {},
   "source": [
    "##### Extending the chain using RunnableLambda\n",
    "query -> chain1 -> (output) -> chain2 -> output -> chain3 -> parser \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0c5d65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "parse_template = PromptTemplate(\n",
    "    input_variables=[\"raw_information\"],\n",
    "    template=\"parse and clean the following customer feedback for key information : \\n {raw_information}\"\n",
    ")\n",
    "\n",
    "sentiment_template = PromptTemplate(\n",
    "    input_variables=[\"key_feedback\"],\n",
    "    template=\"Give one word output as Classify the follwoing key words of a feedback as Positive or Negative : \\n {key_feedback}\"\n",
    ")\n",
    "\n",
    "## RunnableLambda allows us to use custom functions as Runnable components. \n",
    "# This enables functions to be used in chains! To properly format the outputs, \n",
    "# we can create two simple lambda functions for our use case.\n",
    "\n",
    "formated_parsed_output = RunnableLambda(lambda output:{'raw_information':output})\n",
    "formated_classified_output = RunnableLambda(lambda output:{'key_feedback':output})\n",
    "\n",
    "user_feedback = \"The delivery was late, and the product was damaged when it arrived. However, the customer support team was very helpful in resolving the issue quickly.\"\n",
    "\n",
    "chain = parse_template | llm | sentiment_template | llm | StrOutputParser()\n",
    "\n",
    "feedback_sentiment = chain.invoke({\"raw_information\":user_feedback})\n",
    "\n",
    "feedback_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ae00cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_information  :  Here is the key information parsed and cleaned from the customer feedback:\n",
      "\n",
      "**Issue:**\n",
      "\n",
      "* Delivery was late\n",
      "* Product was damaged when it arrived\n",
      "\n",
      "**Resolution:**\n",
      "\n",
      "* Customer support team was helpful\n",
      "* Issue was resolved quickly\n",
      "\n",
      "**Overall sentiment:**\n",
      "\n",
      "* Negative (due to late delivery and damaged product)\n",
      "* Positive (due to helpful customer support team) \n",
      "\n",
      "key_feedback  :  Positive \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### to print the output of middle commponents of chain\n",
    "\n",
    "def print_output(key_name, output):\n",
    "    print(key_name,' : ',output.content,'\\n')\n",
    "    return output.content\n",
    "\n",
    "\n",
    "formated_parsed_output = RunnableLambda(lambda output:print_output(\"raw_information\",output))\n",
    "formated_classified_output = RunnableLambda(lambda output:print_output('key_feedback',output))\n",
    "\n",
    "user_feedback = \"The delivery was late, and the product was damaged when it arrived. However, the customer support team was very helpful in resolving the issue quickly.\"\n",
    "\n",
    "chain = (parse_template \n",
    "        | llm  \n",
    "        | formated_parsed_output \n",
    "        | sentiment_template \n",
    "        | llm \n",
    "        | formated_classified_output\n",
    "        |StrOutputParser())\n",
    "\n",
    "feedback_sentiment = chain.invoke({\"raw_information\":user_feedback})\n",
    "\n",
    "feedback_sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574f092",
   "metadata": {},
   "source": [
    "#### Conditional routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2039a823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_information  :  Here is the parsed and cleaned key information from the customer feedback:\n",
      "\n",
      "* **Issue:** Delivery was late and product was damaged.\n",
      "* **Resolution:** Customer support team was very helpful in resolving the issue quickly.\n",
      "* **Key Takeaway:** Despite the initial issue with delivery and product damage, the customer support team's quick resolution was a positive aspect.\n",
      "\n",
      "Note: I removed the phrase \"However\" as it's a transition word and not essential information. I also combined the two negative points into one issue, and condensed the resolution into one sentence. \n",
      "\n",
      "key_feedback  :  Positive \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here\\'s a draft thank you message:\\n\\n\"Dear [User],\\n\\nWe\\'re thrilled to hear that you had a positive experience with our [service/product]! We\\'re grateful for your feedback and appreciate the time you took to share your thoughts with us.\\n\\nYour feedback is invaluable in helping us improve and provide the best possible experience for our users. We\\'re happy to hear that you enjoyed [specific aspect of service/product] and we\\'ll make sure to pass on your kind words to our team.\\n\\nAs a small token of our appreciation, we\\'d be grateful if you could leave a positive rating on our webpage. Your feedback will help others make informed decisions and we\\'re confident that it will make a big difference to our business.\\n\\nThank you again for your kind words and for choosing [Your Company Name]. We look forward to serving you again in the future.\\n\\nBest regards,\\n[Your Name]\"\\n\\nFeel free to customize it as per your needs!'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for any negative message - send apology msg to user\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def print_output(key_name, output):\n",
    "    print(key_name,' : ',output.content,'\\n')\n",
    "    return output.content\n",
    "\n",
    "\n",
    "parse_template = PromptTemplate(\n",
    "    input_variables=[\"raw_information\"],\n",
    "    template=\"parse and clean the following customer feedback for key information : \\n {raw_information}\"\n",
    ")\n",
    "\n",
    "sentiment_template = PromptTemplate(\n",
    "    input_variables=[\"key_feedback\"],\n",
    "    template=\"Give one word output as Classify the follwoing key words of a feedback as Positive or Negative : \\n {key_feedback}\"\n",
    ")\n",
    "\n",
    "thankyou_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\"],\n",
    "    template=\"Given the feedback, draft a thank you message for the user and request them to leave a positive rating on our webpage:\\n\\n{sentiment}\"\n",
    ")\n",
    "\n",
    "apology_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\"],\n",
    "    template=\"Given the feedback, draft an apology message for the user and mention that their concern has been forwarded to the relevant department:\\n\\n{sentiment}\"\n",
    ")\n",
    "\n",
    "## creating separate chains to be triggered on condition\n",
    "thankyou_chain = thankyou_template | llm | StrOutputParser()\n",
    "apology_chain = apology_template | llm | StrOutputParser()\n",
    "\n",
    "def route(info):\n",
    "    if 'positive' in info['sentiment'].lower():\n",
    "        return thankyou_chain\n",
    "    else:\n",
    "        return apology_chain\n",
    "\n",
    "\n",
    "formated_parsed_output = RunnableLambda(lambda output:print_output(\"raw_information\",output))\n",
    "formated_classified_output = RunnableLambda(lambda output:print_output('key_feedback',output))\n",
    "\n",
    "user_feedback = \"The delivery was late, and the product was damaged when it arrived. However, the customer support team was very helpful in resolving the issue quickly.\"\n",
    "\n",
    "chain = (parse_template \n",
    "        | llm  \n",
    "        | formated_parsed_output \n",
    "        | sentiment_template \n",
    "        | llm \n",
    "        | formated_classified_output\n",
    "        |StrOutputParser())\n",
    "\n",
    "## output of sentiment goes as input to trigger chain\n",
    "feedback_sentiment = chain.invoke({\"raw_information\":user_feedback}) \n",
    "\n",
    "RunnableLambda(route).invoke({'sentiment':feedback_sentiment})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c55e9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_agentic_kn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
